# Data
data:
  data_name: CONRADData
  save_files: True      # Will save files to disk. Will apply proper scaling
  tf_record: False
  # ["aorta", "leftAtrium", "leftVentricle", "myocardium", "rightAtrium", "rightVentricle"]
  components: ["leftVentricle"]  # Must be in constants.py
  phases: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]  # List of values in 0 to 9
  train_interpolation: 'linear'  #  Can be 'linear' or `spline`
  test_interpolation: 'spline'
  fix_time_step:   # either None or a float 0 <= t < 1
  modes: # None to use all modes, otherwise list of existing modes for dataset
  dynamic_modes: # None to use all modes, otherwise list of existing modes for dataset

  batch_size: 1  # take care for out of memory exception
  shuffle_buffer: 1  # no need to use. Dataset is random
  n_prefetch: -1 # Set to -1 for Autotune
  std_shape_generation: [-0.5, 0.5]  # minimum/maximum  [-1, 1] seems too large
  std_shape_test: 0  # Fix float
  mesh_reduction:  # either None, float between 0 and 1 or integer greater 1 (initial shape reduction)
  ds_factors: [2, 2, 2, 2]


training:
  # Training
  test_frequency: 10     # In terms of epochs
  patience: 10

  n_val_samples: 20  # leave at is and already existing data is used
  n_train_samples: 100 # leave at is and already existing data is used
  num_epochs: 50
  num_steps_per_epochs: 100

  # Optimization.
  training_mode: 'joint'  # Could be `alternate` or `joint`
  up_down_reg: 0

  # TODO: alternate each step and not each epoch (not used at the moment)
  alternate_epochs: 1  # Only if `training_mode=alternate`
  up_down_fac: 100000  # Only if `training_mode=joint`
  up_down_decay: 0.9  # Only if `training_mode=joint`

  loss_str: 'l1'
  regularization: 0.001
  dropout: 1
  learning_rate: 0.0005
  decay_rate: 0.99
  momentum: 0.9

testing:
  # model
  experiment: 20200518-135626  # provide time string
  epoch_n: 50

model:
  encoder:
    conv_layers:
      - name: ARMAConv  # Most conv of spektral. params must be adapted below
        channels: 16
        order: 1
        activation: relu
        kernel_regularizer:
          class_name: l1   # Could be l1, l2, or L1L2
          config:
            l: 0.0005
        use_bias: True
      - name: ARMAConv
        channels: 16
        order: 1
        activation: relu
        kernel_regularizer:
          class_name: l1   # Could be l1, l2, or L1L2
          config:
            l: 0.0005
        use_bias: True
      - name: ARMAConv
        channels: 16
        order: 1
        activation: relu
        kernel_regularizer:
          class_name: l1   # Could be l1, l2, or L1L2
          config:
            l: 0.0005
        use_bias: True
      - name: ARMAConv
        channels: 32
        order: 1
        activation: relu
        kernel_regularizer:
          class_name: l1   # Could be l1, l2, or L1L2
          config:
            l: 0.0005
        use_bias: True
    dense_layer:
      name: dense
      units: 10 # Latent dim
      activation: relu
      use_bias: True

    # if the D matrix should be learned (and how)
    learn_down: False
    trainable_down: False  # be careful with collapse to zero latent vector!

  decoder:
    conv_layers:
      - name: ARMAConv
        channels: 32
        order: 1
        activation: relu
        kernel_regularizer:
          class_name: l1   # Could be l1, l2, or L1L2
          config:
            l: 0.0005
        use_bias: True
      - name: ARMAConv
        channels: 16
        order: 1
        activation: relu
        kernel_regularizer:
          class_name: l1   # Could be l1, l2, or L1L2
          config:
            l: 0.0005
        use_bias: True
      - name: ARMAConv
        channels: 16
        order: 1
        activation: relu
        kernel_regularizer:
          class_name: l1   # Could be l1, l2, or L1L2
          config:
            l: 0.0005
        use_bias: True
      - name: ARMAConv
        channels: 3
        order: 1
        activation: relu
        kernel_regularizer:
          class_name: l1   # Could be l1, l2, or L1L2
          config:
            l: 0.0005
        use_bias: True
    dense_layer:
      units:  # None will imply to be computed based on encoder
      name: dense
      activation: relu
      use_bias: True

    # if the U matrix should be learned (and how)
    learn_up: False
    trainable_up: True
